{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started With OpenAI's Toolkit: gym\n",
    "\n",
    "From gym's [documentation](https://gym.openai.com/docs/):\n",
    "\n",
    "> Gym is a toolkit for developing and comparing reinforcement learning algorithms.\n",
    "> The gym library is a collection of test problems — environments — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import scipy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CartPole\n",
    "\n",
    "Next we'll load one of the environments included in the gym toolkit. This environment is called [CartPole](https://gym.openai.com/envs/CartPole-v1/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an environment, 'env', let's see what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now it just a white screen. This is because the render() function only shows one frame, the current state of the environment. Usually we call it in a loop after the environment state has changed.\n",
    "\n",
    "To close the rendered window we can use env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Examine Test Program\n",
    "\n",
    "Now, let's look at the program we used to test our environment, cartpole-test.py\n",
    "\n",
    "The full program is copied in the code block below. Go ahead and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import scipy\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "for episode in range(10):\n",
    "    observation = env.reset()  # Observe the environment\n",
    "    totalreward = 0\n",
    "    for _ in range(200):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()  # Decide how to act\n",
    "        observation, reward, done, info = env.step(action) # Act accordingly\n",
    "        totalreward += reward  # Receive reward/penalty\n",
    "        if done:\n",
    "            break\n",
    "    print(\"Reward for Episode {}: {}\".format(episode, totalreward))\n",
    "    \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program is a basic template for using the gym interface.\n",
    "\n",
    "Note that it follows most of the steps of the learning process for reinforcement learning:\n",
    "1. Observation of the environment\n",
    "2. Deciding how to act (Stategy in this example is randomly choosing)\n",
    "3. Acting accordingly\n",
    "4. Receiving a reward or penalty\n",
    "5. ~~Learning from experiences and refining strategy~~\n",
    "6. ~~Iterate until optimal strategy is found~~\n",
    "\n",
    "Only step 5 and 6 are missing. We'll address that soon.\n",
    "\n",
    "First we will examine steps 1-4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation, Action, Reward\n",
    "\n",
    "In CartPole's environment, there are four observations at any given state, representing information such as the angle of the pole and the position of the cart.\n",
    "\n",
    "Using these observations, the agent needs to decide on one of two possible actions: move the cart left or right.\n",
    "\n",
    "Let's get a better idea of how observations and actions are represented in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "observation = env.reset()  # Observe the environment\n",
    "print(observation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the Agent in the CartPole example \"sees\" the environment solely as these four continous values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "action = env.action_space.sample()  # Decide how to act\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the Agent can take two actions: 0 or 1 (move left or move right)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Without Reinforcement Learning\n",
    "\n",
    "This makes for a relatively simple environment for reinforcement learning. Let's start with something more basic.\n",
    "\n",
    "A simple way to map these observations to an action choice is a linear combination. We define a vector of weights, each weight corresponding to one of the observations. Start off by initializing them randomly between [-1, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = np.random.rand(4) * 2 - 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is the weight vector used? Each weight is multiplied by its respective observation, and the products are summed up. This is equivalent to performing an inner product (matrix multiplication) of the two vectors. If the total is less than 0, we move left. Otherwise, we move right. This is similar to linear regression or a neural network with no hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "\n",
    "# Or expressed as:\n",
    "if np.matmul(parameters,observation) < 0:\n",
    "    action = 0\n",
    "else:\n",
    "    action = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a basic model for choosing actions based on the observations.\n",
    "\n",
    "Next we will want to know how well we're doing. For every timestep we keep the pole straight, we get +1 reward. To estimate how good a given set of weights is, we can just run an episode until the pole drops and see how much reward we got.\n",
    "\n",
    "Let's write a function that can run one episode and report the reward. We'll consider an episode solved if it lasts for 200 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_episode(env, parameters):  \n",
    "    observation = env.reset()\n",
    "    goal = 200\n",
    "    totalreward = 0\n",
    "    for _ in range(goal):\n",
    "        action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")  # Setup fresh environment\n",
    "parameters = np.random.rand(4) * 2 - 1  # Initialize parameters\n",
    "\n",
    "run_episode(env, parameters)  # Run Episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The highest reward possible in one episode is 200.**\n",
    "\n",
    "Now we can measure how well our model performs. The problem to solve is: how can we select the parameters that achieve the highest amount of average reward?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search\n",
    "\n",
    "A straightforward strategy is to keep trying random weights then picking the one that peforms the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search():\n",
    "    num_episodes = 10000\n",
    "    bestparams = None\n",
    "    bestreward = 0\n",
    "    for episodes in range(1, num_episodes+1):  # run 10,000 episodes\n",
    "        parameters = np.random.rand(4) * 2 - 1\n",
    "        reward = run_episode(env, parameters)\n",
    "        if reward > bestreward:  # Check for new personal best\n",
    "            bestreward = reward\n",
    "            bestparams = parameters  # Remember winning parameters\n",
    "            # considered solved if the agent lasts 200 timesteps\n",
    "            if reward == 200:\n",
    "                break\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = random_search()\n",
    "print(\"Solved in {} episodes\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hill-Climbing\n",
    "\n",
    "Another method of choosing parameter weights is the hill-climbing algorithm. We start with some randomly chosen initial weights. Every episode, add some noise to the weights, and keep the new weights if the agent improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hill_climb():\n",
    "    num_episodes = 10000\n",
    "    noise_scaling = 0.1\n",
    "    bestparams = None\n",
    "    bestreward = 0\n",
    "    parameters = np.random.rand(4) * 2 - 1\n",
    "    for episodes in range(1, num_episodes+1):  # run 10,000 episodes\n",
    "        parameters = parameters + (np.random.rand(4) * 2 - 1) * noise_scaling\n",
    "        reward = run_episode(env, parameters)\n",
    "        if reward > bestreward:  # Check for new personal best\n",
    "            bestreward = reward\n",
    "            bestparams = parameters  # Remember winning parameters\n",
    "            # considered solved if the agent lasts 200 timesteps\n",
    "            if reward == 200:\n",
    "                break\n",
    "    return episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = hill_climb()\n",
    "print(\"Solved in {} episodes\".format(count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better Evaluation\n",
    "\n",
    "It appears that the random search strategy typically solves the problem in less than 100 episodes. Does the Hill Climb strategy do better or worse?\n",
    "\n",
    "One way to tell is to plot a histogram of how many episodes it took to solve over man trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_list = []\n",
    "for step in range(100):                   # Run random_search() 1000 times\n",
    "    episodes_list.append(random_search())  # Storing results in episodes_list\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.hist(episodes_list)\n",
    "fig.suptitle('Histogram of Random Search', fontsize=20)\n",
    "plt.xlabel('Episodes required to solve', fontsize=18)\n",
    "plt.ylabel('Frequency', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_list = []\n",
    "for step in range(100):                 # Run random_search() 1000 times\n",
    "    episodes_list.append(hill_climb())  # Storing results in episodes_list\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.hist(episodes_list)\n",
    "fig.suptitle('Histogram of Hill-Climbing Search', fontsize=20)\n",
    "plt.xlabel('Episodes required to solve', fontsize=18)\n",
    "plt.ylabel('Frequency', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the CartPole environment is relatively simple, with only 4 observations, this basic method works surprisingly well. In most cases a more guided strategy like Hill-Climbing does better than random search. It's always good to start with the most simple strategy to use as a benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
