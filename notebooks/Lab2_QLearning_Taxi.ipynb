{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Cab\n",
    "\n",
    "[Taxi](https://gym.openai.com/envs/Taxi-v2/) is one of the environments in the gym toolkit. It is a simulation of a self-driving taxicab in a simplified environment (a 5x5 grid).\n",
    "\n",
    "There are 4 locations (labeled by different letters) and the Taxi's job is to pick up the passenger at one location and drop her off in another.\n",
    "\n",
    "We also want the taxi to:\n",
    "* Drop off passengers at the right location\n",
    "* Deliver the passenger as fast as possible\n",
    "* Take care of the passenger and follow traffic rules\n",
    "\n",
    "To model this as a reinforcement learning solution we'll need to consider: **rewards**, **states**, and **actions**.\n",
    "\n",
    "Recall the learning process for reinforcement learning:\n",
    "1. Observation of the environment\n",
    "2. Deciding how to act\n",
    "3. Acting accordingly\n",
    "4. Receiving a reward or penalty\n",
    "5. Learning from experiences and refining strategy\n",
    "6. Iterate until optimal strategy is found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Rewards\n",
    "\n",
    "Since the **agent** (the Taxi) learns from rewards, we need to understand how the environment rewards and/or penalizes the agent.\n",
    "* The agent receives a high positive reward for a successful dropoff: +20 points\n",
    "* The agent gets a slight negative reward for not making it to the destination every time-step: -1 point\n",
    "* The agent is penalized if it tries to drop-off a passenger in the wrong location: -10 points\n",
    "\n",
    "Luckily, the gym environment takes care of when the agent receives rewards/penalties and how much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. State Space\n",
    "\n",
    "The **State Space**  is is the set of all possible situations the taxi could be in. The state contains useful information the agent needs to make the right action.\n",
    "\n",
    "In this environment we have a 5x5 grid, giving us 25 locations the taxi can inhabit. These 25 locations are part of the **State Space**.\n",
    "\n",
    "Let's take a look at an example of the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "env.s = 242\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just one state of the State Space.\n",
    "\n",
    "We also have four different locations (R, G, Y, B), these are the drop-off locations. \n",
    "\n",
    "In this example state, the Passenger is a drop-off Y (the Passenger location turns purple). The Passenger can be at any of the four drop-off locations or in the taxi, giving her five possible locations.\n",
    "\n",
    "These four drop-off locations and five Passenger locations are part of the State Space as well. In total the taxi environment has, **5 x 5 x 5 x 4 = 500** total possible states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Action Space\n",
    "\n",
    "The agent encounters one of the 500 states and it takes an action. The action in our case can be to move in a direction or decide to pickup/dropoff a passenger.\n",
    "\n",
    "In other words, we have **six possible actions**:\n",
    "0. south\n",
    "1. north\n",
    "2. east\n",
    "3. west\n",
    "4. pickup\n",
    "5. dropoff\n",
    "\n",
    "This is the **Action Space**: the set of all the actions that our agent can take in a given state.\n",
    "\n",
    "Note that the environment also contains walls (represented by the **pipe**, \"|\"). In the environment's code, it will simply provide a -1 penalty for every wall hit and the taxi won't move anywhere. This will rack up penalties causing the taxi to consider going around the wall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Gym's Interface\n",
    "\n",
    "The core gym interface is env, which is the unified environment interface. The following are the env methods that will be helpful:\n",
    "\n",
    "* **env.reset:** Resets the environment and returns a random initial state.\n",
    "* **env.step(action):** Step the environment by one timestep. Returns:\n",
    "  * **observation:** Observations of the environment\n",
    "  * **reward:** If your action was beneficial or not\n",
    "  * **done:** Indicates if we have successfully picked up and dropped off a passenger, also called one episode\n",
    "  * **info:** Additional info such as performance and latency for debugging purposes\n",
    "* **env.render:** Renders one frame of the environment (helpful in visualizing the environment)\n",
    "\n",
    "**Note:** We are using the .env on the end of gym.make() to avoid training stopping at 200 iterations, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify the Action Space and State Space are what we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our RL algorithm won't need any more information than these two things. All we need is a way to identify a state uniquely by assigning a unique number to every possible state, and RL learns to choose an action number from 0-5 where:\n",
    "\n",
    "* 0 = south\n",
    "* 1 = north\n",
    "* 2 = east\n",
    "* 3 = west\n",
    "* 4 = pickup\n",
    "* 5 = dropoff\n",
    "\n",
    "Recall that the 500 states correspond to a encoding of the Taxi's location, the Passenger's location, and the drop-off location.\n",
    "\n",
    "Reinforcement Learning will learn a mapping of states to the optimal action to perform in that state by exploration, i.e. the agent explores the environment and takes actions based off rewards defined in the environment.\n",
    "\n",
    "The optimal action for each state is the action that has the highest cumulative long-term reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Reward Table\n",
    "\n",
    "When the Taxi environment is created, there is an initial Reward table that's also created, called `P`. We can think of it like a matrix that has the number of states as rows and number of actions as columns, i.e. a states × actions matrix.\n",
    "\n",
    "Since every state is in this matrix, we can see the default reward values assigned to our illustration's state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.P[242] # Reward Table for state #242 (out of 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary has the structure:\n",
    "\n",
    "```\n",
    "{action: [(probability, nextstate, reward, done)]}\n",
    "```\n",
    "\n",
    "A few things to note:\n",
    "* The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state.\n",
    "* In this env, probability is always 1.0.\n",
    "* The nextstate is the state we would be in if we take the action at this index of the dict\n",
    "* All the movement actions have a -1 reward and the pickup/dropoff actions have -10 reward in this particular state. If we are in a state where the taxi has a passenger and is on top of the right destination, we would see a reward of 20 at the dropoff action (5)\n",
    "* done is used to tell us when we have successfully dropped off a passenger in the right location. Each successfull dropoff is the end of an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Without Reinforment Learning: Random Search\n",
    "\n",
    "We can try to brute-force our way to solve the problem without RL. Starting with a random search is a good way to benchmark your solutions.\n",
    "\n",
    "We'll create a loop that runs until one passenger reaches their destination (one episode).\n",
    "\n",
    "Recall from the previous lab that env.action_space.sample() method selects one random action from set of all possible actions. We'll have the agent use this to choose actions.\n",
    "\n",
    "**Note:** Sine we create the environment with \"make(\"Taxi-v2\")\" rather than \"make(\"Taxi-v2\").env\", it will stop running after 200 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"Taxi-v2\")\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "done = False\n",
    "\n",
    "env.reset()\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it would be nice to see what the agent is doing, we've stored each frame in the 'frame' list. Let's write a function that can replay these frames for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'].getvalue())\n",
    "        print(\"Timestep: {}\".format(i+1))\n",
    "        print(\"State: {}\".format(frame['state']))\n",
    "        print(\"Action: {}\".format(frame['action']))\n",
    "        print(\"Reward: {}\".format(frame['reward']))\n",
    "        sleep(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can pass a list of frames to the print_frames() function to replay an episode.\n",
    "\n",
    "**Note: It may take a while to replay all the frames if the episode took many timesteps. You can stop the cell from running by interrupting the kernel. To do so: Click the stop button; or go to Kernel-> Interrupt **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function that can run one episode of taking random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns:\n",
    "frames - A list of frames from every timestep\n",
    "epochs - The number of timesteps taken\n",
    "penalties - The total number of penalties received\n",
    "'''\n",
    "def run_random_episode(env):\n",
    "    epochs = 0\n",
    "    penalties, reward = 0, 0\n",
    "\n",
    "    frames = [] # for animation\n",
    "    done = False\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    return frames, epochs, penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v2\").env # create environment without 200 timestep cut-off\n",
    "_, timesteps, penalties = run_random_episode(env) # run one episode\n",
    "\n",
    "print(\"Timesteps taken: {}\".format(timesteps))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a random search function that can run many episodes so we can more easily evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns:\n",
    "timesteps - The number of timesteps taken\n",
    "penalties - The total number of penalties received\n",
    "'''\n",
    "def random_search(num_episodes):\n",
    "    num_episodes = num_episodes\n",
    "    time_list = []\n",
    "    penalty_list = []\n",
    "    \n",
    "    for episodes in range(1, num_episodes+1):\n",
    "        _, timesteps, penalties = run_random_episode(env)\n",
    "        time_list.append(timesteps)\n",
    "        penalty_list.append(penalties)\n",
    "        \n",
    "    return time_list, penalty_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Random Search\n",
    "\n",
    "Let's start by running random search 100 times. We'll need matplot and numpy for processing and viewing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## %% time is a system call                                     ##\n",
    "## It will display how much time it took to run this code block ##\n",
    "## This may take ~30 to 60 seconds                              ##\n",
    "\n",
    "\n",
    "times, penalties = random_search(100)  # Run random_search() 100 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot histograms for timesteps and penalties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.hist(times)\n",
    "fig.suptitle('Histogram of Timesteps for Random Search', fontsize=20)\n",
    "plt.xlabel('Episodes required to solve', fontsize=18)\n",
    "plt.ylabel('Frequency', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.hist(penalties)\n",
    "fig.suptitle('Histogram of Penalties for Random Search', fontsize=20)\n",
    "plt.xlabel('Episodes required to solve', fontsize=18)\n",
    "plt.ylabel('Frequency', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's also calculate the mean values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_time = np.array(times).mean()\n",
    "mean_penalty = np.array(penalties).mean()\n",
    "\n",
    "print(\"Mean timesteps taken: {}\".format(mean_time))\n",
    "print(\"Mean penalties incurred: {}\".format(mean_penalty))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving with RL: Q-Learning\n",
    "\n",
    "Essentially, Q-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state.\n",
    "\n",
    "In our Taxi environment, we have the reward table, P, that the agent will learn from. It does this by looking at the reward for taking an action in the current state, then updating a Q-value to remember if that action was beneficial.\n",
    "\n",
    "These Q-values are stored in the Q-table. They map to a (state, action) combination.\n",
    "\n",
    "A Q-value for a particular state-action combination is representative of the \"quality\" of an action taken from that state. Better Q-values imply better chances of getting greater rewards.\n",
    "\n",
    "For example, if the taxi is faced with a state that includes a passenger at its current location, it is highly likely that the Q-value for pickup is higher when compared to other actions, like dropoff or north.\n",
    "\n",
    "Q-values are initialized to an arbitrary value, and as the agent exposes itself to the environment and receives different rewards by executing different actions, the Q-values are updated using the equation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Q({\\small state}, {\\small action}) \\leftarrow (1 - \\alpha) Q({\\small state}, {\\small action}) + \\alpha \\Big({\\small reward} + \\gamma \\max_{a} Q({\\small next \\ state}, {\\small all \\ actions})\\Big)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- α (alpha) is the learning rate (0<α≤1) - Just like in supervised learning settings, α is the extent to which our Q-values are being updated in every iteration.\n",
    "\n",
    "- γ (gamma) is the discount factor (0≤γ≤1) - determines how much importance we want to give to future rewards. A high value for the discount factor (close to __1__) captures the long-term effective award, whereas, a discount factor of **0** makes our agent consider only immediate reward, hence making it greedy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is this saying?**\n",
    "\n",
    "We are assigning (←), or updating, the Q-value of the agent's current state and action by first taking a weight (1−α) of the old Q-value, then adding the learned value. The learned value is a combination of the reward for taking the current action in the current state, and the discounted maximum reward from the next state we will be in once we take the current action.\n",
    "\n",
    "Basically, we are learning the proper action to take in the current state by looking at the reward for the current state/action combo, and the max rewards for the next state. This will eventually cause our taxi to consider the route with the best rewards strung together.\n",
    "\n",
    "The Q-value of a state-action pair is the sum of the instant reward and the discounted future reward (of the resulting state). The way we store the Q-values for each state and action is through a Q-table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Table\n",
    "\n",
    "The Q-table is a matrix where we have a row for every state (500) and a column for every action (6). It's first initialized to 0, and then values are updated after training. Note that the Q-table has the same dimensions as the reward table, but it has a completely different purpose.\n",
    "\n",
    "### Summary of Q-Learning Process\n",
    "\n",
    "Breaking it down into steps, we get\n",
    "\n",
    "* Initialize the Q-table by all zeros.\n",
    "* Start exploring actions: For each state, select any one among all possible actions for the current state (S).\n",
    "* Travel to the next state (S') as a result of that action (a).\n",
    "* For all possible actions from the state (S') select the one with the highest Q-value.\n",
    "* Update Q-table values using the equation.\n",
    "* Set the next state as the current state.\n",
    "* If goal state is reached, then end and repeat the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploiting learned values\n",
    "\n",
    "After enough random exploration of actions, the Q-values tend to converge serving our agent as an action-value function which it can exploit to pick the most optimal action from a given state.\n",
    "\n",
    "There's a tradeoff between exploration (choosing a random action) and exploitation (choosing actions based on already learned Q-values). We want to prevent the action from always taking the same route, and possibly overfitting, so we'll be introducing another parameter called **ϵ** \"epsilon\" to cater to this during training.\n",
    "\n",
    "Instead of just selecting the best learned Q-value action, we'll sometimes favor exploring the action space further. Lower epsilon value results in episodes with more penalties (on average) because we are exploring and making random decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Agent\n",
    "\n",
    "First, we'll initialize the Q-table to a 500×6 matrix of zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "print(q_table.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the training algorithm that will update this Q-table as the agent explores the environment over thousands of episodes.\n",
    "\n",
    "In the first part of *while not done*, we decide whether to pick a random action or to exploit the already computed Q-values. This is done simply by using the *epsilon* value and comparing it to the *random.uniform(0, 1)* function, which returns an arbitrary number between 0 and 1.\n",
    "\n",
    "We execute the chosen action in the environment to obtain the *next_state* and the *reward* from performing the action. After that, we calculate the maximum Q-value for the actions corresponding to the *next_state*, and with that, we can easily update our Q-value to the *new_q_value*.\n",
    "\n",
    "Now we will train the agent for 100,000 episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "        next_state, reward, done, info = env.step(action) \n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Episode: {}\".format(i))\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the agent\n",
    "\n",
    "Let's evaluate the performance of our agent. We don't need to explore actions any further, so now the next action is always selected using the best Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "    all_epochs.append(epochs)\n",
    "    all_penalties.append(penalties)\n",
    "\n",
    "print(\"Results after {} episodes:\".format(episodes))\n",
    "print(\"Average timesteps per episode: {}\".format(total_epochs / episodes))\n",
    "print(\"Average penalties per episode: {}\".format(total_penalties / episodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how this compares to the random search strategy, let's plot out histograms of timesteps and penalties again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.hist(all_epochs)\n",
    "fig.suptitle('Histogram of Timesteps for Random Search', fontsize=20)\n",
    "plt.xlabel('Episodes required to solve', fontsize=18)\n",
    "plt.ylabel('Frequency', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.hist(all_penalties)\n",
    "fig.suptitle('Histogram of Penalties for Random Search', fontsize=20)\n",
    "plt.xlabel('Episodes required to solve', fontsize=18)\n",
    "plt.ylabel('Frequency', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The Q-Learning strategy can easily solve an episode in less than 20 timesteps **and** it rarely ever gets penalized. This performance is way better than what we saw with the random search approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write your own program\n",
    "\n",
    "Next, I want you to use this code as a reference to write your own program that can solve a different environment: Acrobot-v1\n",
    "\n",
    "You can write the program in a jupyter notebook (.ipynb) or in a stand-alone python script (.py), whichever you prefer.\n",
    "\n",
    "You should:\n",
    "* Write a function that can do N steps of a random search method\n",
    "* Write a function that can do N steps of a Q-Learning method\n",
    "* Evaluate the two by comparing how many timesteps they take to solve the problem\n",
    "\n",
    "To get started, he is some information about the State and Action Space of the Acrobot-v1 environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Acrobot-v1\")\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "# Observation: 6 continuous values\n",
    "print(observation)\n",
    "print(env.observation_space)\n",
    "\n",
    "# Actions: 3 discrete values\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
