{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Exercise in Discrete Space Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve the Frozen Lake Problem\n",
    "\n",
    "In this lab you will implement a solution for the frozen lake problem. To help you get started we'll review the Taxi problem solution from Lab 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference the Taxi Solution\n",
    "\n",
    "The training function is defined below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle # for exporting and importing datastructure files\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Training function using Q-Learning\n",
    "'''\n",
    "Arguments:\n",
    "    q_table - The Q-Table to use for training\n",
    "    num_episdoes - Number of episodes to train\n",
    "    alpha - learning rate\n",
    "    gamma - discount factor\n",
    "    epsilon - exploration rate\n",
    "'''\n",
    "def taxi_train(q_table, num_episodes, alpha, gamma, epsilon, verbose=False):\n",
    "    # Initialize the Taxi Environment\n",
    "    env = gym.make(\"Taxi-v2\").env\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Training for {} episodes\".format(num_episodes))\n",
    "\n",
    "    for i in range(1, num_episodes+1):\n",
    "        state = env.reset()\n",
    "\n",
    "        epochs, penalties, reward = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        while not done:  # Run until episode is solved\n",
    "\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample() # Explore action space\n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) # Exploit learned values\n",
    "    \n",
    "            next_state, reward, done, info = env.step(action) \n",
    "            \n",
    "            old_value = q_table[state, action]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "            \n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state, action] = new_value\n",
    "    \n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "    \n",
    "            state = next_state\n",
    "            epochs += 1\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(\"\\rEpisode: {}\".format(i))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nTraining finished\\n\")\n",
    "        \n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now we can initialize the model variables:\n",
    "* Number of episodes\n",
    "* Q-Table\n",
    "* Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Taxi Environment\n",
    "env = gym.make(\"Taxi-v2\").env\n",
    "\n",
    "# Initialize the Q-Table\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n]) # (500, 6) matrix\n",
    "\n",
    "# Initialize Number of Episodes\n",
    "train_duration = 10000\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then pass these variables to the train function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_train(q_table, train_duration, alpha, gamma, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Now we need a function to evaluate the model. For evaluation we will no longer be chaning the Q-Table values, rather we will only use it to decide what action to make (all 'exploitation' no 'exploration'). Below is the code we used from Lab 2. I've also added two functions for plotting histograms of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "'''\n",
    "Arguments:\n",
    "    q_table - A trained Q-Table\n",
    "    num_episodes - Number of episodes to run the simulation\n",
    "Returns:\n",
    "    all_rewards - A list of the total rewards received per episode, one value for each episode\n",
    "    all_penalties - A list of the total penalties received per episode, one value for each episode\n",
    "'''\n",
    "def taxi_evaluate(q_table, num_episodes, verbose=False):\n",
    "    # Initialize the Taxi Environment\n",
    "    env = gym.make(\"Taxi-v2\").env\n",
    "    \n",
    "    all_rewards = []\n",
    "    all_penalties = []\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Evaluating on {} episodes\".format(num_episodes))\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        epochs, penalties, reward = 0, 0, 0\n",
    "        totalreward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state])\n",
    "            state, reward, done, info = env.step(action)\n",
    "    \n",
    "            totalreward += reward\n",
    "    \n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "    \n",
    "            epochs += 1\n",
    "            if epochs >= 5000: # Cap an episode at 5k timesteps\n",
    "                break\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(\"\\rEpisode: {}\".format(i))\n",
    "   \n",
    "        all_rewards.append(totalreward)\n",
    "        all_penalties.append(penalties)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nEvaluating finished\\n\")\n",
    "        print(\"Mean rewards: {}\".format(np.array(all_rewards).mean()))\n",
    "        print(\"Mean penalties: {}\".format(np.array(all_penalties).mean()))\n",
    "        \n",
    "    env.close()\n",
    "    return all_rewards, all_penalties\n",
    "\n",
    "\n",
    "def plot_timesteps_histogram(rewards_list):\n",
    "    fig = plt.figure()\n",
    "    plt.hist(rewards_list)\n",
    "    fig.suptitle('Histogram of Timesteps for Taxi Q-Learning', fontsize=20)\n",
    "    plt.xlabel('Rewards Received', fontsize=18)\n",
    "    plt.ylabel('Frequency', fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "def plot_penalties_histogram(penalties_list):\n",
    "    fig = plt.figure()\n",
    "    plt.hist(penalties_list)\n",
    "    fig.suptitle('Histogram of Penalties for Taxi Q-Learning', fontsize=20)\n",
    "    plt.xlabel('Penalties Received', fontsize=18)\n",
    "    plt.ylabel('Frequency', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards, penalties = taxi_evaluate(q_table, 1000, True)\n",
    "plot_timesteps_histogram(rewards)\n",
    "plot_penalties_histogram(penalties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the learned Q-Table for external use\n",
    "pickle.dump(q_table, open(\"taxi_q_table.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render an Episode\n",
    "\n",
    "It would be nice to see how the agent behaves during an episode so we'll define a run function that saves each frame and plays it back. Similar to the evaluate function, it doesn't update the Q-Table anymore, just exploits what it has learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "# Run function\n",
    "'''\n",
    "Arguments:\n",
    "    q_table - A trained Q-Table\n",
    "'''\n",
    "def taxi_run(q_table, verbose=False):\n",
    "    # Initialize the Taxi Environment\n",
    "    env = gym.make(\"Taxi-v2\").env\n",
    "    \n",
    "    frames = []\n",
    "\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "            \n",
    "    # Finished: Print frames\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'].getvalue())\n",
    "        print(\"Timestep: {}\".format(i+1))\n",
    "        print(\"State: {}\".format(frame['state']))\n",
    "        print(\"Action: {}\".format(frame['action']))\n",
    "        print(\"Reward: {}\".format(frame['reward']))\n",
    "        sleep(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_run(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Create a Q-Table\n",
    "\n",
    "To get started, look at the observation space and action space. Use the taxi example as a reference.\n",
    "\n",
    "Only modify the code between the comments:\n",
    "\n",
    "\\## Your code starts here ##\n",
    "\n",
    "\\## Your code ends here ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Taxi Environment\n",
    "env = gym.make(\"FrozenLake-v0\").env\n",
    "\n",
    "# Initialize the Q-Table\n",
    "## Your code starts here ##\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n]) # (16, 4) matrix\n",
    "## Your code ends here ##\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Decide What Action To Take\n",
    "\n",
    "There are 3 sections for you to modify in the cell block below.\n",
    "\n",
    "Again, only modify the code between the comments:\n",
    "\n",
    "\\## Your code starts here ##\n",
    "\n",
    "\\## Your code ends here ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle # for exporting and importing datastructure files\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Training function using Q-Learning\n",
    "'''\n",
    "Arguments:\n",
    "    q_table - The Q-Table to use for training\n",
    "    num_episdoes - Number of episodes to train\n",
    "    alpha - learning rate\n",
    "    gamma - discount factor\n",
    "    epsilon - exploration rate\n",
    "'''\n",
    "def frozenlake_train(q_table, num_episodes, alpha, gamma, epsilon, verbose=False):\n",
    "    # Initialize the Taxi Environment\n",
    "    env = gym.make(\"FrozenLake-v0\").env\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Training for {} episodes\".format(num_episodes))\n",
    "\n",
    "    for i in range(1, num_episodes+1):\n",
    "        state = env.reset()\n",
    "\n",
    "        epochs, penalties, reward = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        while not done:  # Run until episode is solved\n",
    "\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                ## Your code starts here ##\n",
    "                action = env.action_space.sample() # Explore action space\n",
    "                ## Your code ends here ##\n",
    "            else:\n",
    "                ## Your code starts here ##\n",
    "                action = np.argmax(q_table[state]) # Exploit learned values\n",
    "                ## Your code ends here ##\n",
    "    \n",
    "            ## Your code starts here ##\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            ## Your code ends here ##\n",
    "        \n",
    "            if done and reward < 1:    # if fell in hole (finished without a reward of 1)\n",
    "                reward = -10          # Give big negative reward so Q-Values learn from mistake\n",
    "                penalties += 1\n",
    "                \n",
    "            elif done and reward >= 1:    # if found goal (finished with a reward of 1)\n",
    "                reward = 1000          # Give big positive reward\n",
    "                \n",
    "            else:                     # if did not win or fail yet\n",
    "                reward = -1         # Give small negative reward to encourage finishing fast\n",
    "            \n",
    "            old_value = q_table[state, action]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "            \n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state, action] = new_value\n",
    "    \n",
    "            state = next_state\n",
    "            epochs += 1\n",
    "            if epochs >= 2000: # Cap an episode at 2k timesteps\n",
    "                break\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(\"\\rEpisode: {}\".format(i))\n",
    "            print(\"\\rReward: {}\".format(reward))\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nTraining finished\\n\")\n",
    "        \n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Number of Episodes\n",
    "train_duration = 100000\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_table.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozenlake_train(q_table, train_duration, alpha, gamma, epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Exploit for Evaluation\n",
    "\n",
    "During evaluation we no longer want to explore random actions. Instead we want to act solely by using the trained Q-Table. Fill in the code below to make the agent choose its action accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "'''\n",
    "Arguments:\n",
    "    q_table - A trained Q-Table\n",
    "    num_episodes - Number of episodes to run the simulation\n",
    "Returns:\n",
    "    all_rewards - A list of the number of timesteps it took to solve one episode, one value for each episode\n",
    "    all_penalties - A list of the total penalties received per episode, one value for each episode\n",
    "'''\n",
    "def frozenlake_evaluate(q_table, num_episodes, verbose=False):\n",
    "    # Initialize the Taxi Environment\n",
    "    env = gym.make(\"FrozenLake-v0\").env\n",
    "    \n",
    "    all_rewards = []\n",
    "    all_penalties = []\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Evaluating on {} episodes\".format(num_episodes))\n",
    "    for i in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        epochs, penalties, reward = 0, 0, 0\n",
    "        totalreward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            ## Your code starts here ##\n",
    "            action = np.argmax(q_table[state])\n",
    "            state, reward, done, info = env.step(action)\n",
    "            ## Your code ends here ##\n",
    "\n",
    "            if done and reward < 1:    # if fell in hole (finished without a reward of 1)\n",
    "                reward = -10          # Give big negative reward so Q-Values learn from mistake\n",
    "                penalties += 1\n",
    "                \n",
    "            elif done and reward >= 1:    # if found goal (finished with a reward of 1)\n",
    "                reward = 1000          # Give big positive reward\n",
    "                \n",
    "            else:                     # if did not win or fail yet\n",
    "                reward = -1         # Give small negative reward to encourage finishing fast\n",
    "            \n",
    "            \n",
    "            totalreward += reward\n",
    "    \n",
    "            epochs += 1\n",
    "            if epochs >= 2000: # Cap an episode at 2k timesteps\n",
    "                break\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(\"\\rEpisode: {}\".format(i))\n",
    "   \n",
    "        all_rewards.append(totalreward)\n",
    "        all_penalties.append(penalties)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\nEvaluating finished\\n\")\n",
    "        print(\"Mean reward: {}\".format(np.array(all_rewards).mean()))\n",
    "        print(\"Mean penalties: {}\".format(np.array(all_penalties).mean()))\n",
    "        \n",
    "    env.close()\n",
    "    return all_rewards, all_penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_timesteps_histogram(rewards_list):\n",
    "    fig = plt.figure()\n",
    "    plt.hist(rewards_list)\n",
    "    fig.suptitle('Histogram of Rewards for FrozenLake Q-Learning', fontsize=20)\n",
    "    plt.xlabel('Rewards Received', fontsize=18)\n",
    "    plt.ylabel('Frequency', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_penalties_histogram(penalties_list):\n",
    "    fig = plt.figure()\n",
    "    plt.hist(penalties_list)\n",
    "    fig.suptitle('Histogram of Penalties for FrozenLake Q-Learning', fontsize=20)\n",
    "    plt.xlabel('Penalties Receives', fontsize=18)\n",
    "    plt.ylabel('Frequency', fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times, penalties = frozenlake_evaluate(q_table, 1000, True)\n",
    "plot_timesteps_histogram(times)\n",
    "plot_penalties_histogram(penalties)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Render an Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "\n",
    "# Run function\n",
    "'''\n",
    "Arguments:\n",
    "    q_table - A trained Q-Table\n",
    "'''\n",
    "def frozenlake_run(q_table, verbose=False):\n",
    "    # Initialize the Taxi Environment\n",
    "    env = gym.make(\"FrozenLake-v0\").env\n",
    "    \n",
    "    frames = []\n",
    "\n",
    "    state = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        clear_output(wait=True)\n",
    "        env.render()\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Put each rendered frame into dict for animation\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "            }\n",
    "        )\n",
    "            \n",
    "    # Finished: Print frames\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'].getvalue())\n",
    "        print(\"Timestep: {}\".format(i+1))\n",
    "        print(\"State: {}\".format(frame['state']))\n",
    "        print(\"Action: {}\".format(frame['action']))\n",
    "        print(\"Reward: {}\".format(frame['reward']))\n",
    "        sleep(.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozenlake_run(q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
