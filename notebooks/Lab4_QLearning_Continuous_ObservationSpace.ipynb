{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning for Continuous Observation Spaces\n",
    "\n",
    "Previously we looked at solving the Taxi problem with Q-Learning. The core idea behind Q-Learning is creating a table, the Q-Table, the will represent what the agent has learned over time.\n",
    "\n",
    "The Q-table is a matrix where we have a row for every state and a column for every action. It's first initialized to 0, and then values are updated during training.\n",
    "\n",
    "The agent learns by looking at the reward for taking an action in the current state, then updating a Q-value to remember if that action was beneficial.\n",
    "\n",
    "These Q-values are stored in the Q-table. They map to a (state, action) combination.\n",
    "\n",
    "A Q-value for a particular state-action combination is representative of the \"quality\" of an action taken from that state. Better Q-values imply better chances of getting greater rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem\n",
    "\n",
    "In order to build a Q-Table with a row for every possible state and a column for every possible action, the states and actions have to be countable (discrete). However, many real-world situations are not discrete, they are represented by continuous values. This means there are literally an infinite number of possible states.\n",
    "\n",
    "So how can we use Q-Learning to solve continuous problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Solution\n",
    "\n",
    "One way to solve this problem is to map the range of continuous values to a discrete number of \"buckets\". This is known as quantization: [wikipedia](https://en.wikipedia.org/wiki/Quantization_(signal_processing)).\n",
    "\n",
    "We can use quantization to map our continuos inputs to any number of countable states, however many we think we may need. For this case we'll call these countable states \"buckets\". The fewer buckets we have, the less accurate our model will be. At the same time, we don't want too many buckets because our Q-Table could become too large.\n",
    "\n",
    "For example, having 10 million buckets to represent the environment state would give us high accuracy but the Q-Table would have 10 million rows multiplied by how many possible actions there are. This would require a lot of memory to store the Q-Table and probably hinder performance.\n",
    "\n",
    "So the goal becomes: Find a reasonable number of buckets that can still accurately represent the observation space.\n",
    "\n",
    "How do we know if we are accurately representing the observation space? We see if the agent can solve the task at hand. If the Q-Learning agent cannot solve the problem, we probably need to give it more information by breaking the inputs into more buckets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole\n",
    "\n",
    "Let's investigate solving a problem with a continuous observation space, the CartPole problem.\n",
    "\n",
    "Previously we saw solutions to the CartPole problem without using reinforcement learning. We looked at random search and a simplistic hill-climbing appoarch. The random search solution did better so we'll start by using that as are benchmark to compare our Q-Learning solution to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of Gym's Interface\n",
    "\n",
    "The core gym interface is env, which is the unified environment interface. The following are the env methods that will be helpful:\n",
    "\n",
    "* **env.reset:** Resets the environment and returns a random initial state.\n",
    "* **env.step(action):** Step the environment by one timestep. Returns:\n",
    "  * **observation:** Observations of the environment\n",
    "  * **reward:** If your action was beneficial or not\n",
    "  * **done:** Indicates if we have successfully picked up and dropped off a passenger, also called one episode\n",
    "  * **info:** Additional info such as performance and latency for debugging purposes\n",
    "* **env.render:** Renders one frame of the environment (helpful in visualizing the environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by looking at the observation space and action space for this environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 4 continuos variables for the observation space and 2 discrete variables for the action space. It's typically useful to know the range of the values you are working with.\n",
    "\n",
    "In gym, if you are working with a continuous space (observation or action) then you can call '.low' and '.high' to get the lowest values and highest values, respectively.\n",
    "\n",
    "When working in a discrete space then you can call '.n' to get the number of discrete variables there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "print(\"Observation Space:\") # We know the observation space is continuous so we'll use '.low' and '.high' to get the range\n",
    "print(\"> Lowest values: {}\".format(env.observation_space.low))\n",
    "print(\"> Highest values: {}\".format(env.observation_space.high))\n",
    "print(\"> Range: {}\".format(env.observation_space.high - env.observation_space.low))\n",
    "\n",
    "print(\"\\nAction Space:\") # We know the action space is discrete so we'll use '.n' to get the range\n",
    "print(\"> Range: {}\".format(env.action_space.n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the range for the second and fourth observation variables is infinity. To make this work with Q-Learning we'll have to come up with more reasonable bounds. We will worry about that soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Search Solution\n",
    "\n",
    "As before we will be doing a linear combination appoarch, creating one weight parameter for every observation variable we have. Then we'll randomly pick new parameters until we find a set that can solve the problem.\n",
    "\n",
    "Below we define a function that performs random search for the CartPole problem. The code is slightly modified from Lab 1. This time we can change the goal to be achieved by the agent.\n",
    "\n",
    "We also add a 'render_episode' function that we can use to see how well a trained agent performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Arguments:\n",
    "    env - An instance of the CartPole environment\n",
    "    num_episodes - Number of episodes to run environment\n",
    "    goal - Number of timesteps for Agent to last to be considered a success (default is 200)\n",
    "'''\n",
    "def random_search(env, num_episodes, goal=200):\n",
    "    parameters = np.random.rand(4) * 2 - 1  # Initialize parameters\n",
    "\n",
    "    bestparams = None\n",
    "    bestreward = 0\n",
    "    \n",
    "    for episodes in range(1, num_episodes+1):  # run 'num_episodes' episodes\n",
    "        parameters = np.random.rand(4) * 2 - 1\n",
    "        \n",
    "        observation = env.reset()\n",
    "        totalreward = 0\n",
    "        for _ in range(goal):\n",
    "            action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "            observation, reward, done, info = env.step(action)\n",
    "            totalreward += reward\n",
    "            if done:\n",
    "                break\n",
    "        reward = totalreward\n",
    "\n",
    "        if reward > bestreward:  # Check for new personal best\n",
    "            bestreward = reward\n",
    "            bestparams = parameters  # Remember winning parameters\n",
    "            # considered solved if the agent lasts 'goal' timesteps\n",
    "            # Stop training if solved\n",
    "            if reward >= goal:\n",
    "                break\n",
    "    env.close() # Cleanup environment\n",
    "    return episodes, bestreward, bestparams\n",
    "\n",
    "def run_episode(env, parameters, goal=200, render=False):\n",
    "    observation = env.reset()\n",
    "    totalreward = 0\n",
    "    for _ in range(goal):\n",
    "        if(render):\n",
    "            env.render()\n",
    "        action = 0 if np.matmul(parameters,observation) < 0 else 1\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        totalreward += reward\n",
    "        if done:\n",
    "            break\n",
    "    env.close() # Cleanup environment\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run random search for 2000 episodes and see if it can solve the problem. This time rather than a goal of lasting for 200 timesteps, we'll make it a little more challenging. The agent will have to last for 1,000 timesteps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\").env  # Setup fresh environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e, r, p = random_search(env, 2000, goal=1000)\n",
    "print(\"Took {} episodes to solve.\".format(e))\n",
    "print(\"Parameters used to solve: {}\".format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can render an episode using the parameters stored in the variable 'p'. We'll set the goal to 500 for rendering so it doesn't run for too long. Doing 500 timesteps should take ~10 to ~20 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "env = gym.make(\"CartPole-v1\").env  # Setup fresh environment\n",
    "r = run_episode(env, p, goal=500, render=True)\n",
    "print(\"Earned reward of {}\".format(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time to evaluate the model, let's see how well it generalizes to a harder problem.\n",
    "\n",
    "We trained it one lasting for 1,000 timesteps. If it has generalized well then it should be able to last much longer than that. For this test we will set the goal to 2,000 timesteps. We will evaluate it by seeing how many times out of 100 it passes this test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = []\n",
    "passed_count = 0\n",
    "test_goal = 2000\n",
    "for step in range(100):  # Run test 100 times\n",
    "    r = run_episode(env, p, goal=test_goal, render=False)\n",
    "    if r >= test_goal:\n",
    "        passed_count += 1\n",
    "    reward_list.append(r)  # Storing results in reward_list\n",
    "    \n",
    "print(\"Model passed the test with a success rate of {}%\".format(passed_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.hist(reward_list)\n",
    "fig.suptitle('Histogram of Random Search', fontsize=20)\n",
    "plt.xlabel('Reward Earned', fontsize=18)\n",
    "plt.ylabel('Frequency', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Solution\n",
    "\n",
    "To get started with a Q-Learning approach we need to come up with a way to quantinize the input. Normally you would have to experiment a bit to see how many buckets you should use to represent the state. For this demonstration we'll use values that I found to work out well.\n",
    "\n",
    "Recall that there are four variables. These variables represent: position of the cart, velocity of the cart, angle of the pole, and angluar velocity of the pole. We'll call these: (x, x', theta, theta'). The table below shows how many buckets we'll use for each variable.\n",
    "\n",
    "| Variable | Number of Buckets\n",
    "| :- | -:\n",
    "| **x** | 1\n",
    "| **x'** | 1\n",
    "| **theta** | 6\n",
    "| **theta'** | 3\n",
    "\n",
    "Below we come up with a convenient way to store this information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the \"Cart-Pole\" environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Number of discrete states (bucket) per state dimension\n",
    "NUM_BUCKETS = (2, 2, 6, 3)  # (x, x', theta, theta')\n",
    "\n",
    "# Number of discrete actions\n",
    "NUM_ACTIONS = env.action_space.n # (left, right)\n",
    "\n",
    "# Index of the action\n",
    "ACTION_INDEX = len(NUM_BUCKETS)\n",
    "\n",
    "# Bounds for each discrete state\n",
    "STATE_BOUNDS = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "## This gives us a list of (low, high) pairs. In other words, the range for each variable\n",
    "## STATE_BOUNDS[0]: x\n",
    "## STATE_BOUNDS[1]: x'\n",
    "## STATE_BOUNDS[2]: theta\n",
    "## STATE_BOUNDS[3]: theta'\n",
    "\n",
    "print(STATE_BOUNDS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from earlier that we noticed x' and theta' have a range of infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Range for x': {}\".format(STATE_BOUNDS[1][0] - STATE_BOUNDS[1][1]))\n",
    "print(\"Range for theta': {}\".format(STATE_BOUNDS[3][0] - STATE_BOUNDS[3][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To avoid this infinity problem, we'll change the bounds to something more reasonable for these velocity values. Think of these values as the largest extremes we would need to consider for this environment. Coming up with these values is somewhat arbitrary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STATE_BOUNDS[1] = [-0.5, 0.5]\n",
    "STATE_BOUNDS[3] = [-math.radians(50), math.radians(50)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined our buckets and boundaries for the continuous variables, we can define a function that looks at a set of those values and decides what bucket each variable value falls into.\n",
    "\n",
    "This function may be pretty confusing but don't worry too much about it. Understanding programming logic like this only comes with a lot of practice. For the purposes of this class, just trust this function places values into buckets accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_to_bucket(state):\n",
    "    bucket_indice = []\n",
    "    for i in range(len(state)): # For every variable in the observation space:\n",
    "        if state[i] <= STATE_BOUNDS[i][0]:    # if the value is less than or equal to the lower bound\n",
    "            bucket_index = 0                  # put it the first bucket\n",
    "        elif state[i] >= STATE_BOUNDS[i][1]:  # if the value is greater than or equal to the higher bound\n",
    "            bucket_index = NUM_BUCKETS[i] - 1 # put it in the last bucket\n",
    "        else:                                 # otherwise: determine which bucket the value falls into.\n",
    "            # Mapping the state bounds to the bucket array\n",
    "            bound_width = STATE_BOUNDS[i][1] - STATE_BOUNDS[i][0]\n",
    "            offset = (NUM_BUCKETS[i]-1)*STATE_BOUNDS[i][0]/bound_width\n",
    "            scaling = (NUM_BUCKETS[i]-1)/bound_width\n",
    "            bucket_index = int(round(scaling*state[i] - offset))\n",
    "        bucket_indice.append(bucket_index)\n",
    "    return tuple(bucket_indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at an example to see how the 'state_to_bucket' function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the \"Cart-Pole\" environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "state = env.reset()\n",
    "buckets = state_to_bucket(state)\n",
    "print(state)\n",
    "print(buckets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method of mapping the continuous variables to discrete states. With this we can implement Q-Learning as we did with the taxi problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Agent\n",
    "\n",
    "The code below defines the state variables we were talking about. It also defines a few helper functions to make training easier. The 'q_learning_train' function contains the base Q-Learning process. It is very similiar to the Taxi Q-Learning solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "## Initialize the \"Cart-Pole\" environment\n",
    "env = gym.make('CartPole-v0').env\n",
    "\n",
    "## Defining the environment related constants\n",
    "\n",
    "# Number of discrete states (bucket) per state dimension\n",
    "NUM_BUCKETS = (1, 1, 6, 3)  # (x, x', theta, theta')\n",
    "# Number of discrete actions\n",
    "NUM_ACTIONS = env.action_space.n # (left, right)\n",
    "# Bounds for each discrete state\n",
    "STATE_BOUNDS = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "STATE_BOUNDS[1] = [-0.5, 0.5]\n",
    "STATE_BOUNDS[3] = [-math.radians(50), math.radians(50)]\n",
    "# Index of the action\n",
    "ACTION_INDEX = len(NUM_BUCKETS)\n",
    "\n",
    "## Creating a Q-Table for each state-action pair\n",
    "q_table = np.zeros(NUM_BUCKETS + (NUM_ACTIONS,))\n",
    "\n",
    "## Learning related constants\n",
    "MIN_EXPLORE_RATE = 0.001\n",
    "MIN_LEARNING_RATE = 0.01\n",
    "\n",
    "## Defining the simulation related constants\n",
    "NUM_EPISODES = 1000\n",
    "STREAK_TO_END = 120\n",
    "\n",
    "## Eval metrics\n",
    "all_rewards = []\n",
    "\n",
    "def q_learning_train(env, num_episodes, goal=200):\n",
    "\n",
    "    ## Instantiating the learning related parameters\n",
    "    learning_rate = get_learning_rate(0)\n",
    "    explore_rate = get_explore_rate(0)\n",
    "    discount_factor = 0.99  # since the world is unchanging\n",
    "\n",
    "    num_streaks = 0\n",
    "\n",
    "    for episode in range(1, num_episodes+1):\n",
    "\n",
    "        # Reset the environment\n",
    "        obv = env.reset()\n",
    "\n",
    "        # the initial state\n",
    "        state_0 = state_to_bucket(obv)\n",
    "        \n",
    "        totalreward = 0\n",
    "\n",
    "        for t in range(1, goal+1):\n",
    "            #env.render()\n",
    "\n",
    "            # Select an action\n",
    "            action = select_action(state_0, explore_rate)\n",
    "\n",
    "            # Execute the action\n",
    "            obv, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Observe the result\n",
    "            state = state_to_bucket(obv)\n",
    "\n",
    "            totalreward += reward\n",
    "            \n",
    "            # Update the Q based on the result\n",
    "            best_q = np.amax(q_table[state])\n",
    "            q_table[state_0 + (action,)] += learning_rate*(reward + discount_factor*(best_q) - q_table[state_0 + (action,)])\n",
    "\n",
    "            # Setting up for the next iteration\n",
    "            state_0 = state\n",
    "\n",
    "            if done:\n",
    "                if (t >= goal):\n",
    "                    num_streaks += 1\n",
    "                else:\n",
    "                    num_streaks = 0\n",
    "                break\n",
    "                \n",
    "        if episode%100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Episode {}/{} finished with reward: {}\".format(episode, num_episodes, totalreward))\n",
    "\n",
    "        # It's considered done when it's solved over 120 times consecutively\n",
    "        if num_streaks > STREAK_TO_END:\n",
    "            print(\"Solved 120 times consecutively after {} episodes\".format(episode))\n",
    "            break\n",
    "\n",
    "        # Update parameters\n",
    "        explore_rate = get_explore_rate(episode)\n",
    "        learning_rate = get_learning_rate(episode)\n",
    "        \n",
    "        all_rewards.append(totalreward)\n",
    "\n",
    "    return episode, all_rewards\n",
    "\n",
    "def select_action(state, explore_rate):\n",
    "    # Select a random action\n",
    "    if random.random() < explore_rate:\n",
    "        action = env.action_space.sample()\n",
    "    # Select the action with the highest q\n",
    "    else:\n",
    "        action = np.argmax(q_table[state])\n",
    "    return action\n",
    "\n",
    "\n",
    "def get_explore_rate(t):\n",
    "    return max(MIN_EXPLORE_RATE, min(1, 1.0 - math.log10((t+1)/25)))\n",
    "\n",
    "def get_learning_rate(t):\n",
    "    return max(MIN_LEARNING_RATE, min(0.5, 1.0 - math.log10((t+1)/25)))\n",
    "\n",
    "def state_to_bucket(state):\n",
    "    bucket_indice = []\n",
    "    for i in range(len(state)):\n",
    "        if state[i] <= STATE_BOUNDS[i][0]:\n",
    "            bucket_index = 0\n",
    "        elif state[i] >= STATE_BOUNDS[i][1]:\n",
    "            bucket_index = NUM_BUCKETS[i] - 1\n",
    "        else:\n",
    "            # Mapping the state bounds to the bucket array\n",
    "            bound_width = STATE_BOUNDS[i][1] - STATE_BOUNDS[i][0]\n",
    "            offset = (NUM_BUCKETS[i]-1)*STATE_BOUNDS[i][0]/bound_width\n",
    "            scaling = (NUM_BUCKETS[i]-1)/bound_width\n",
    "            bucket_index = int(round(scaling*state[i] - offset))\n",
    "        bucket_indice.append(bucket_index)\n",
    "    return tuple(bucket_indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these functions defined, we can now actually train the agent. It will take a bit longer than the random search function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "episode_count, rewards = q_learning_train(env, 1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average reward per episode: {}\".format(np.array(rewards).mean()))\n",
    "print(\"Max reward per episode: {}\".format(np.array(rewards).max()))\n",
    "print(\"Min reward per episode: {}\".format(np.array(rewards).min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the trained Q-Table so we don't have to train again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_q_table = q_table\n",
    "print(trained_q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the agent\n",
    "\n",
    "Let's evaluate the performance of our agent. We don't need to explore actions any further, so now the next action is always selected using the best Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_q_episode(env, q_table, goal=200, render=False):\n",
    "\n",
    "    # Reset the environment\n",
    "    obv = env.reset()\n",
    "\n",
    "    # the initial state\n",
    "    state_0 = state_to_bucket(obv)\n",
    "    \n",
    "    totalreward = 0\n",
    "\n",
    "    for t in range(goal):\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "        # Select an action\n",
    "        action = np.argmax(q_table[state_0])\n",
    "\n",
    "        # Execute the action\n",
    "        obv, reward, done, _ = env.step(action)\n",
    "\n",
    "        # Observe the result\n",
    "        state = state_to_bucket(obv)\n",
    "        \n",
    "        totalreward += reward\n",
    "\n",
    "        # Setting up for the next iteration\n",
    "        state_0 = state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    env.close()\n",
    "    return totalreward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the \"Cart-Pole\" environment\n",
    "env = gym.make('CartPole-v0').env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_list = []\n",
    "passed_count = 0\n",
    "test_goal = 2000\n",
    "for step in range(100):  # Run test 100 times\n",
    "    r = run_q_episode(env, trained_q_table, goal=test_goal, render=False)\n",
    "    if r >= test_goal:\n",
    "        passed_count += 1\n",
    "    reward_list.append(r)  # Storing results in reward_list\n",
    "    \n",
    "print(\"Model passed the test with a success rate of {}%\".format(passed_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.hist(reward_list)\n",
    "fig.suptitle('Histogram of Q-Learning', fontsize=20)\n",
    "plt.xlabel('Reward Earned', fontsize=18)\n",
    "plt.ylabel('Frequency', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method seems to typically earn a success rate of about 20% to 25%.\n",
    "\n",
    "Surprisingly, the random search method still out-performs the Q-Learning method we set up. We could improve on this model but that requires more advanced concepts that we haven't covered yet. Unfortunately, just training the model for longer barely gives us any performance gains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "So now you have seen how to solve two different finite-space problems using Q-Learning and a solution (although not too impressive) for a continuous-space problem. As you've probably noticed, solving continuous-space problems is much more complex. This is a class of problems known as \"Control Problems\" - spawned from the field of robotics.\n",
    "\n",
    "We used quantization, a more manual approach, to solve the problem of continuous-space. However there is another approach that uses a neural network to map the continuous inputs to a set of finite states, or \"buckets\". This neural network approach for Q-Learning is called a \"Q-Network\".\n",
    "\n",
    "Understanding how a Q-Network works requires understanding a few more topics that we haven't covered in this class. However, many people present their Q-Network solutions to various gym environments online. I encourage you in the future to look for these solutions and see if you can apply them to other environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
